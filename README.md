# MinRL: Minimal, Clean Code for Reinforcement Learning

MinRL provides clean, minimal implementations of fundamental reinforcement learning algorithms in a customizable GridWorld environment. The project focuses on educational clarity and implementation simplicity while maintaining production-quality code standards.

## ğŸŒŸ Key Features

- **Modular GridWorld Environment**: Customizable grid sizes (3Ã—3, 5Ã—5) with configurable rewards and dynamics
- **Clean Implementation** of core RL algorithms:
  - Policy Evaluation (Bellman Expectation)
  - Policy Iteration & Value Iteration
  - Tabular Q-Learning
  - Deep Q-Learning (DQN)
- **Visualization Tools**: Built-in plotting and state-value visualization
- **Comprehensive Tests**: 100% test coverage with pytest
- **Educational Focus**: Well-documented code with step-by-step examples


## ğŸ“ Project Structure

```
minrl/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ environment/       
â”‚   â”‚   â””â”€â”€ grid_world.py  # Core grid world logic, state transitions, rewards, and environment dynamics
â”‚   â”œâ”€â”€ agents/           
â”‚   â”‚   â”œâ”€â”€ policy_evaluation.py  # Implements Bellman Expectation for policy evaluation
â”‚   â”‚   â”œâ”€â”€ policy_optimization.py  # Policy iteration and value iteration implementations
â”‚   â”‚   â”œâ”€â”€ q_learning.py  # Tabular Q-Learning algorithm
â”‚   â”‚   â””â”€â”€ deep_q_learning.py  # Deep Q-Learning (DQN) implementation using neural networks
â”‚   â””â”€â”€ utils/             
â”‚       â””â”€â”€ visualization.py  # Visualizes state values, learned policies, and rewards over episodes
â”œâ”€â”€ tests/  # Comprehensive test suite ensuring correct functionality
â”œâ”€â”€ examples/              
â”‚   â”œâ”€â”€ basic_navigation.py  # Basic navigation example using a static GridWorld
â”‚   â””â”€â”€ run_experiments.py  # Runs experiments with all implemented RL algorithms
â””â”€â”€ docs/  # Implementation Guide for Beginners

```

## ğŸ“ Implemented Algorithms

1. **Policy Evaluation**
   - Implements Bellman expectation equation
   - Iterative value computation
   - Convergence validation

2. **Policy/Value Iteration**
   - Value iteration with Bellman optimality
   - Policy iteration (evaluation + improvement)
   - Optimal policy extraction

3. **Q-Learning**
   - Tabular state-action value learning
   - Îµ-greedy exploration
   - Hyperparameter tuning support

4. **Deep Q-Learning (DQN)**
   - Neural network function approximation
   - Experience replay memory
   - Target network implementation
   - PyTorch backend

## ğŸ› ï¸ Dependencies

```bash
- Python 3.7+
- NumPy >= 1.19.0
- PyTorch >= 1.8.0
- Matplotlib >= 3.3.0
- Seaborn >= 0.11.0
```

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/10-OASIS-01/minrl.git
cd minrl

# Create a virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: .\venv\Scripts\activate

# Install the package
pip install -e .
```

### **Basic Usage**

Hereâ€™s how you can start using the basic functionality of the library:

```python
from minrl.environment import GridWorld
from minrl.agents import DQNAgent

# Create a 3x3 GridWorld environment
env = GridWorld(size=3)

# Initialize and train a DQN agent
agent = DQNAgent(env)
rewards = agent.train(episodes=1000)

# Visualize the learned policy
agent.visualize_policy()
```

This code sets up a simple **3x3 GridWorld**, trains a **DQN agent** for 1000 episodes, and then visualizes the learned policy.


### **Run Basic Navigation Example**

The **basic_navigation.py** example demonstrates the agent navigating a static GridWorld environment. To run this example:

```bash
# Run the basic navigation example
python -m minrl.examples.basic_navigation
```

This will execute a basic navigation task where the agent tries to learn how to move in the 3x3 GridWorld. It will show how to set up the environment and train a simple agent.


### **Run Experiments with All Implemented Algorithms**

To see how the different RL algorithms (like Q-learning, DQN, etc.) perform on the same environment, you can run **run_experiments.py**. This will execute a full set of experiments and compare the results.

```bash
# Run experiments with all RL algorithms
python -m minrl.examples.run_experiments
```

This command will automatically run experiments for all available RL algorithms, training agents using Q-Learning, Deep Q-Learning, and other approaches. The results will allow you to compare the performance of each algorithm in the same GridWorld environment.


## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request


åœ¨Acknowledgmentséƒ¨åˆ†åŠ å…¥å¯¹Shiyu Zhaoè€å¸ˆçš„æ„Ÿè°¢æ—¶ï¼Œä½ å¯ä»¥è¿™æ ·å†™ï¼š

---

## âœ¨ Acknowledgments

- Inspired by [CleanRL](https://github.com/vwxyzjn/cleanrl) and [RLCode](https://github.com/rlcode/reinforcement-learning)
- 
- Special thanks to Professor Shiyu Zhao for his insightful course on the "Mathematical Foundations of Reinforcement Learning," which provided a solid foundation for my understanding of reinforcement learning. The course materials, including the textbook, PPTs, and code, can be found on his [GitHub repository](https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning), and the English [lecture videos](https://www.youtube.com/playlist?list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8) and Chinese [lecture videos](https://space.bilibili.com/2044042934/lists/748665?type=season) are available online.


## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.


## ğŸ”— Contact

Created by: Yibin Liu  

Email: [yibin.leon.liu@outlook.com](yibin.leon.liu@outlook.com)  

Last Updated: 2025-02-08

